{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Neural Networks and Backpropagation\n",
    "In this assignment, you will be asked to write your own code to implement the learning process of a simple neural network. We will use a simple version of [MNIST dataset](http://yann.lecun.com/exdb/mnist/), which we have introduced in Assignment 5. To make the problem simpler, we only take images with label '8' and '9', which gives us a binary classification problem. Then we subsample the dataset and reduce the dimension of each image using average pooling. The following code loads the dataset and prints its dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 964)\n",
      "(196, 414)\n",
      "(964,)\n",
      "(414,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "#Load data\n",
    "import scipy.io as sio\n",
    "a = sio.loadmat('mnist_binary.mat')\n",
    "X_trn = a['X_trn']\n",
    "X_tst = a['X_tst']\n",
    "Y_trn = a['Y_trn'][0]\n",
    "Y_tst = a['Y_tst'][0]\n",
    "print(X_trn.shape)\n",
    "print(X_tst.shape)\n",
    "print(Y_trn.shape)\n",
    "print(Y_tst.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "1. You are not allowed to use any machine learning libraries which have neural networks implemented.\n",
    "\n",
    "2. Notice here most of the problems you have will be regarding the dimensions of variables. In each skeleton function we provide, we have one assert line to help you verify whether you write your code correctly. Passing the assert line doesn't mean your code is correct. But it is a necessary condition.\n",
    "\n",
    "3. You don't need to strictly follow the skeleton we provide. As long as you answer the problems correctly, you can write in any style you prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Let's first implement a simple neural network with one hidden layer and one output layer. The hidden layer only has $n_h$ neurons. We assume the output layer has two neurons. Hence you will have 4 parameters to describe the neural network: \n",
    "\n",
    "1. $W_1$, a $n_h$ by 196 (number of features) matrix, which is the weight matrix between features and the hidder layer.\n",
    "2. $b_1$, a scalar, which is the offset for the first layer.\n",
    "3. $W_2$, a 2 by $n_h$ matrix, which is the weight matrix between the hidder layer and the output layer.\n",
    "4. $b_2$, a scalar, which is the offset for the second layer.\n",
    "\n",
    "The following script initializes the above four parameters and returns them as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (20, 196)\n",
      "b1 (20, 1)\n",
      "W2 (2, 20)\n",
      "b2 (2, 1)\n"
     ]
    }
   ],
   "source": [
    "#Initialize parameters \n",
    "num_hidden = 20 #number of neurons in the hidden layer\n",
    "num_op = 2 #number of neurons in the output layer\n",
    "\n",
    "def initialize_parameters(size_input, size_hidden, size_output):\n",
    "    np.random.seed(2)\n",
    "    W1 = np.random.randn(size_hidden, size_input) * 0.01\n",
    "    b1 = np.zeros(shape=(size_hidden, 1))\n",
    "    W2 = np.random.randn(size_output, size_hidden) * 0.01\n",
    "    b2 = np.zeros(shape=(size_output, 1))\n",
    "    parameters = {'W1': W1,\n",
    "                  'b1': b1,\n",
    "                  'W2': W2,\n",
    "                  'b2': b2}\n",
    "    return parameters\n",
    "parameters = initialize_parameters(X_trn.shape[0], num_hidden, num_op)\n",
    "print('W1',parameters['W1'].shape)\n",
    "print('b1',parameters['b1'].shape)\n",
    "print('W2',parameters['W2'].shape)\n",
    "print('b2',parameters['b2'].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function.\n",
    "Let $Z_2=(z_1, z_2)$ be the final otuput layer of neurons. The softmax outputs are probability estimates for outputing label 1 (assuming '8' is 1 and '9' is zero):\n",
    "\n",
    "$$\\hat{y}_1 = Pr(Y = 1 | z_1, z_2) = \\frac{e^{z_1}}{e^{z_1} + e^{z_2}}$$\n",
    "\n",
    "Write code in the cell below to do the softmax computation. Note here Z2 should be a matrix of shape $2 \\times n$, where $n$ is the number of training samples. Your output softmax should be of shape $1 \\times n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(Z2):\n",
    "    # ip - (M,N) array where M is no. of neurons in output layer, N is number of samples.\n",
    "    # You can modify the code if your output layer is of different dimension\n",
    "   # =========Write your code below ==============\n",
    "    n = Z2.shape[1]\n",
    "    softmax = np.zeros(shape=(1, n))\n",
    "    def sm(x,y):\n",
    "        return np.exp(x)/(np.exp(x)+np.exp(y))\n",
    "    \n",
    "    smfunc = np.vectorize(sm)\n",
    "    softmax[0] = smfunc(Z2[0],Z2[1])\n",
    "    # =============================================\n",
    "    assert(softmax.shape == (1, Z2.shape[1]))\n",
    "    return softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function.\n",
    "The following function should be able to implement activation function given the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activ(ip,act):\n",
    "    # ip - array obtained after multiplying inputs with weights (between input layer and hidden layer)\n",
    "    # act - ReLU or Sigmoid\n",
    "    # I am assuming that \"ip\" already includes the bias terms, since the bias terms were not separately passed as a parameter\n",
    "    out = np.zeros(shape=ip.shape)\n",
    "    \n",
    "    def m(x):\n",
    "        return np.maximum(x,0)\n",
    "    def s(x):\n",
    "        return 1.0/(1+np.exp(-1*x))\n",
    "    \n",
    "    if act ==\"ReLU\":\n",
    "        # =========Write your code below ==============\n",
    "        f = np.vectorize(m)\n",
    "        out = m(ip)\n",
    "\n",
    "    # =============================================\n",
    "    elif act == \"Sigmoid\":\n",
    "        # =========Write your code below ==============\n",
    "        f = np.vectorize(s)\n",
    "        out = f(ip)\n",
    "                \n",
    "\n",
    "    # =============================================\n",
    "    assert(out.shape == ip.shape)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Definition of \"ip\" </font> ##\n",
    "<font color='blue'> I decided to include the bias term inside \"ip\", as using the global variable b1 outside of the \"activ\" function is programmatically unsound. Instead, the bias term was added to the product of the input and hidden layer in the forward propagation code below. Functionally, it should be no different and works as expected. This is just a technicallity point for ensuring code readability and debugging.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "Given $X, W_1, b_1, W_2, b_2$, the following function will compute the neurons and activated values in the hidden layer, denoted by $Z_1, A_1$ respectively. It will also return the neurons in the last layer and the softmax function computed from it, denoted by $Z_2, A_2$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward Propagation   \n",
    "def forward_propagation(X, parameters, act):\n",
    "# =========Write your code below ==============\n",
    "    #Z1 = W1*X+b1\n",
    "    Z1 = parameters['W1'].dot(X) + parameters['b1']\n",
    "    A1 = activ(Z1,act)\n",
    "    Z2 = parameters['W2'].dot(A1) + parameters['b2']\n",
    "    A2 = softmax(Z2)\n",
    "    # =============================================\n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    neuron = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    return neuron\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation\n",
    "In this assignment, we will use the cross-entropy loss defined below as our loss function. Recall that $Z_2=(z_1, z_2)$ is the final layer of neurons, and after softmax we obtain $\\hat{y}$ which corresponds to the probability of label 8. Let $y$ be the true labels (assume 1 for '8', 0 for '9')\n",
    "$$L(y,\\hat{y}) = -y\\log(\\hat{y}) - (1-y)\\log(1-\\hat{y}),$$\n",
    "where $$\\hat{y} = \\frac{e^{z_1}}{e^{z_1} + e^{z_2}}.$$\n",
    "\n",
    "You have shown in the assignment that:\n",
    "$$\\frac{\\partial L(y,\\hat{y})}{\\partial z_1} = \\hat{y} - y, \\frac{\\partial L(y,\\hat{y})}{\\partial z_2} = y - \\hat{y}.$$\n",
    "Given the parameters and the neuron values, we can calculate the derivative of the loss function w.r.t all the parameters $W_1, b_1, W_2, b_2$ using backward propagation. Note here, all the gradients should be of the same dimension as the corresponding parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(parameters, neuron, X, Y, act):\n",
    "# =========Write your code below ==============\n",
    "    # neuron = {\"Z1\": Z1,\n",
    "    #         \"A1\": A1,\n",
    "    #         \"Z2\": Z2,\n",
    "    #         \"A2\": A2}\n",
    "    # logistic function\n",
    "    def sigm(x):\n",
    "        return 1.0/(1+np.exp(-1*x))\n",
    "    def sigmaderiv(x):\n",
    "        return (1.0/(1+np.exp(-1*x)))*(1-1.0/(1+np.exp(-1*x)))\n",
    "    # heaviside function, derivative of the ReLU activation function\n",
    "    def heavy(x):\n",
    "        return np.heaviside(x, 0)\n",
    "    \n",
    "    Z1 = neuron[\"Z1\"]\n",
    "    Z2 = neuron[\"Z2\"]\n",
    "    A1 = neuron[\"A1\"]\n",
    "    A2 = neuron[\"A2\"]\n",
    "    W2 = parameters['W2']\n",
    "    W1 = parameters['W1']\n",
    "    dldz2 = np.zeros(shape=Z2.shape)\n",
    "    \n",
    "    def dldz2top(x,y):\n",
    "        return x - y\n",
    "    def dldz2bottom(x,y):\n",
    "        return y - x\n",
    "    topfunc = np.vectorize(dldz2top)\n",
    "    bottomfunc = np.vectorize(dldz2bottom)\n",
    "    top = topfunc(A2[0],Y)\n",
    "    bottom = bottomfunc(A2[0],Y)\n",
    "    dldz2 = np.vstack((top,bottom))\n",
    "\n",
    "    # gradients computation\n",
    "    dW2 = dldz2.dot(A1.transpose())\n",
    "    db2 = dldz2.dot(np.ones(shape=(X.shape[1],1)))\n",
    "    # da1dz1 will depend on the activation function\n",
    "    if act == \"Sigmoid\":\n",
    "        da1dz1 = np.zeros(shape=Z1.shape)\n",
    "        s = np.vectorize(sigmaderiv)\n",
    "        da1dz1 = s(Z1)\n",
    "        dW1 = (np.multiply((((dldz2.transpose()).dot(W2)).transpose()),da1dz1)).dot(X.transpose())\n",
    "        db1 = (np.multiply((((dldz2.transpose()).dot(W2)).transpose()),da1dz1)).dot(np.ones(shape=(X.shape[1],1)))\n",
    "    if act == \"ReLU\":\n",
    "        da1dz1 = np.zeros(shape=Z1.shape)\n",
    "        g = np.vectorize(heavy)\n",
    "        da1dz1 = g(Z1)\n",
    "        dW1 = (np.multiply((((dldz2.transpose()).dot(W2)).transpose()),da1dz1)).dot(X.transpose())\n",
    "        db1 = (np.multiply((((dldz2.transpose()).dot(W2)).transpose()),da1dz1)).dot(np.ones(shape=(X.shape[1],1)))\n",
    "    # =============================================\n",
    "    \n",
    "    assert(dW1.shape == W1.shape)\n",
    "    assert(dW2.shape == W2.shape)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "# neuron = forward_propagation(X_trn, parameters, act=\"Sigmoid\")\n",
    "# print(backprop(parameters, neuron, X_trn, Y_trn, act='Sigmoid')['dW1'])\n",
    "# print(backprop(parameters, neuron, X_trn, Y_trn, act='Sigmoid')['dW2'])\n",
    "# print(backprop(parameters, neuron, X_trn, Y_trn, act='Sigmoid')['db1'])\n",
    "# print(backprop(parameters, neuron, X_trn, Y_trn, act='Sigmoid')['db2'])\n",
    "\n",
    "\n",
    "def cross_entropy_loss(softmax, Y):\n",
    "# =========Write your code below ==============\n",
    "    #L(y,y') = -ylog(y') - (1-y)log(1-y')\n",
    "    loss = np.zeros(shape=Y.shape)\n",
    "    def entropy(y,yhat):\n",
    "        return -y*np.log(yhat) - (1-y)*np.log(1-yhat)\n",
    "    e = np.vectorize(entropy)\n",
    "    loss = e(Y,softmax[0])\n",
    "# =============================================        \n",
    "    assert(loss.shape == Y.shape)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter updates\n",
    "Given the parameters and the gradients, we simply update the parameters by the following:\n",
    "\n",
    "$$W = W - \\eta dW$$\n",
    "\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "# =========Write your code below ==============\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    W1 = W1 - learning_rate*grads['dW1']\n",
    "    W2 = W2 - learning_rate*grads['dW2']\n",
    "    b1 = b1 - learning_rate*grads['db1']\n",
    "    b2 = b2 - learning_rate*grads['db2']\n",
    "\n",
    "# =============================================\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network models\n",
    "Combining the above mentioned parameters, implement the following function to learn a neural network and do inference on it. For prediction, you take the argument that gives the largest softmax output at the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def nn_model1(X_trn, X_tst, Y_trn, Y_tst, n_h, n_o, epochs, act, learning_rate):\n",
    "    #X_trn: the training set\n",
    "    #X_tst: the test set\n",
    "    #Y_trn: training labels\n",
    "    #Y_tst: test labels\n",
    "    #n_h: number of neurons in the hidden layer\n",
    "    #n_o: number of neurons in the output layer\n",
    "    #epochs: number of epochs for the training\n",
    "    #act: the activation function you choose\n",
    "    #learning_rate: a list of length epochs, which consists of the learning rate in each step\n",
    "    \n",
    "    def predict(Y, x, parameters, act):\n",
    "        pred = np.zeros(shape=Y.shape)\n",
    "        z1 = parameters['W1'].dot(x) + parameters['b1']\n",
    "        a1 = activ(z1,act)\n",
    "        z2 = parameters['W2'].dot(a1) + parameters['b2']\n",
    "        a2 = np.zeros(shape=(1, z2.shape[1]))                  \n",
    "        for i in range(0,z2.shape[1]):\n",
    "            a2[0][i] = np.exp(z2[0][i])/(np.exp(z2[0][i])+np.exp(z2[1][i]))\n",
    "            \n",
    "        for i in range(0,len(a2[0])):\n",
    "            if a2[0][i] >= 0.5:\n",
    "                pred[i] = 1\n",
    "            else:\n",
    "                pred[i] = 0\n",
    "        return pred\n",
    "    \n",
    "    assert(len(learning_rate) == epochs)\n",
    "    err_tst = []\n",
    "    err_trn = []\n",
    "    loss_trn = []\n",
    "    parameters = initialize_parameters(X_trn.shape[0], n_h, n_o)\n",
    "    \n",
    "   # =========Write your code below ==============\n",
    "    for i in range(0,epochs):\n",
    "        print(i)\n",
    "        neuron = forward_propagation(X_trn, parameters, act)\n",
    "        loss = np.sum(cross_entropy_loss(neuron[\"A2\"], Y_trn))/len(Y_trn)\n",
    "        loss_trn.append(loss)\n",
    "        grads = backprop(parameters, neuron, X_trn, Y_trn, act)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate[i])\n",
    "        trn_pred = predict(Y_trn,X_trn,parameters,act)\n",
    "        tst_pred = predict(Y_tst,X_tst,parameters,act)\n",
    "        print(\"Train error: \")\n",
    "        print(1-accuracy_score(Y_trn, trn_pred))\n",
    "        err_trn.append(1-accuracy_score(Y_trn, trn_pred))\n",
    "        err_tst.append(1-accuracy_score(Y_tst, tst_pred))\n",
    "        \n",
    "    # =============================================    \n",
    "    #err_tst: testing error (classification error) in each epoch\n",
    "    #err_trn: training error (classification error) in each epoch\n",
    "    #loss_trn: training loss (cross entropy loss) in each epoch\n",
    "    #parameters: the final learned parameters\n",
    "    return err_tst, err_trn, loss_trn, parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0: Verify that your code is working well.\n",
    "Using ReLU (Sigmoid) as your activation function, implement a learning algorithm with fixed learning rate $\\eta = 0.01$ at each step. Set the number of epochs to be 20000. Plot the cross entropy loss at each epoch to convince yourself that you are training well. (Your cross entropy loss should be decreasing smoothly. This part won't be graded.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Scaling the Learning Rate </font> ##\n",
    "<font color='blue'> I added a scaling factor of 1/964 to each of the learning rates in the problems below, as suggested per Ziteng Sun's piazza post.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train error: \n",
      "0.39419087136929465\n",
      "1\n",
      "Train error: \n",
      "0.421161825726141\n",
      "2\n",
      "Train error: \n",
      "0.446058091286307\n",
      "3\n",
      "Train error: \n",
      "0.4595435684647303\n",
      "4\n",
      "Train error: \n",
      "0.48755186721991706\n",
      "5\n",
      "Train error: \n",
      "0.49377593360995853\n",
      "6\n",
      "Train error: \n",
      "0.49377593360995853\n",
      "7\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "8\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "9\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "10\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "11\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "12\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "13\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "14\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "15\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "16\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "17\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "18\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "19\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "20\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "21\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "22\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "23\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "24\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "25\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "26\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "27\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "28\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "29\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "30\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "31\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "32\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "33\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "34\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "35\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "36\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "37\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "38\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "39\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "40\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "41\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "42\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "43\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "44\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "45\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "46\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "47\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "48\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "49\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "50\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "51\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "52\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "53\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "54\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "55\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "56\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "57\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "58\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "59\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "60\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "61\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "62\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "63\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "64\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "65\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "66\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "67\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "68\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "69\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "70\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "71\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "72\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "73\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "74\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "75\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "76\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "77\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "78\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "79\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "80\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "81\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "82\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "83\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "84\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "85\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "86\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "87\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "88\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "89\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "90\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "91\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "92\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "93\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "94\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "95\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "96\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "97\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "98\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "99\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "100\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "101\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "102\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "103\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "104\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "105\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "106\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "107\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "108\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "109\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "110\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "111\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "112\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "113\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "114\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "115\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "116\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "117\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "118\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "119\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "120\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "121\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "122\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "123\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "124\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "125\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "126\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "127\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "128\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "129\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "130\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "131\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "132\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "133\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "134\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "135\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "136\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "137\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "138\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "139\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "140\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "141\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "142\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "143\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "144\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "145\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "146\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "147\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "148\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "149\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "150\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "151\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "152\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "153\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "154\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "155\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "156\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "157\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "158\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "159\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "160\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "161\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "162\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "163\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "164\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "165\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "166\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "167\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "168\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "169\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "170\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "171\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "172\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "173\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "174\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "175\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "176\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "177\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "178\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "179\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "180\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "181\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "182\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "183\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "184\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "185\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "186\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "187\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "188\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "189\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "190\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "191\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "192\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "193\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "194\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "195\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "196\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "197\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "198\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "199\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "200\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "201\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "202\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "203\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "204\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "205\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "206\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "207\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "208\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "209\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "210\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "211\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "212\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "213\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "214\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "215\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "216\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "217\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "218\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "219\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "220\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "221\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "222\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "223\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "224\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "225\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "226\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: \n",
      "0.4927385892116183\n",
      "228\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "229\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "230\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "231\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "232\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "233\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "234\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "235\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "236\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "237\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "238\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "239\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "240\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "241\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "242\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "243\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "244\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "245\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "246\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "247\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "248\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "249\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "250\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "251\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "252\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "253\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "254\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "255\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "256\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "257\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "258\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "259\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "260\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "261\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "262\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "263\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "264\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "265\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "266\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "267\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "268\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "269\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "270\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "271\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "272\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "273\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "274\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "275\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "276\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "277\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "278\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "279\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "280\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "281\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "282\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "283\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "284\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "285\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "286\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "287\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "288\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "289\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "290\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "291\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "292\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "293\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "294\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "295\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "296\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "297\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "298\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "299\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "300\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "301\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "302\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "303\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "304\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "305\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "306\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "307\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "308\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "309\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "310\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "311\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "312\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "313\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "314\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "315\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "316\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "317\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "318\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "319\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "320\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "321\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "322\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "323\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "324\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "325\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "326\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "327\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "328\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "329\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "330\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "331\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "332\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "333\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "334\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "335\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "336\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "337\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "338\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "339\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "340\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "341\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "342\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "343\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "344\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "345\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "346\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "347\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "348\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "349\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "350\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "351\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "352\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "353\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "354\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "355\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "356\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "357\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "358\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "359\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "360\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "361\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "362\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "363\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "364\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "365\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "366\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "367\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "368\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "369\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "370\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "371\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "372\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "373\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "374\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "375\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "376\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "377\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "378\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "379\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "380\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "381\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "382\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "383\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "384\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "385\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "386\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "387\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "388\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "389\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "390\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "391\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "392\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "393\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "394\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "395\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "396\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "397\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "398\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "399\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "400\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "401\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "402\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "403\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "404\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "405\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "406\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "407\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "408\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "409\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "410\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "411\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "412\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "413\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "414\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "415\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "416\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "417\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "418\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "419\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "420\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "421\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "422\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "423\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "424\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "425\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "426\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "427\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "428\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "429\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "430\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "431\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "432\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "433\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "434\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "435\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "436\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "437\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "438\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "439\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "440\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "441\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "442\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "443\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "444\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "445\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "446\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "447\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "448\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "449\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "450\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: \n",
      "0.4927385892116183\n",
      "452\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "453\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "454\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "455\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "456\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "457\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "458\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "459\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "460\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "461\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "462\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "463\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "464\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "465\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "466\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "467\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "468\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "469\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "470\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "471\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "472\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "473\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "474\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "475\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "476\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "477\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "478\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "479\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "480\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "481\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "482\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "483\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "484\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "485\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "486\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "487\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "488\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "489\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "490\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "491\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "492\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "493\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "494\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "495\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "496\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "497\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "498\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "499\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "500\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "501\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "502\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "503\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "504\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "505\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "506\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "507\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "508\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "509\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "510\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "511\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "512\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "513\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "514\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "515\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "516\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "517\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "518\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "519\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "520\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "521\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "522\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "523\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "524\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "525\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "526\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "527\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "528\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "529\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "530\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "531\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "532\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "533\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "534\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "535\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "536\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "537\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "538\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "539\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "540\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "541\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "542\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "543\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "544\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "545\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "546\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "547\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "548\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "549\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "550\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "551\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "552\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "553\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "554\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "555\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "556\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "557\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "558\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "559\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "560\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "561\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "562\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "563\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "564\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "565\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "566\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "567\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "568\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "569\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "570\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "571\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "572\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "573\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "574\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "575\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "576\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "577\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "578\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "579\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "580\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "581\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "582\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "583\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "584\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "585\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "586\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "587\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "588\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "589\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "590\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "591\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "592\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "593\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "594\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "595\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "596\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "597\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "598\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "599\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "600\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "601\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "602\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "603\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "604\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "605\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "606\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "607\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "608\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "609\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "610\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "611\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "612\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "613\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "614\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "615\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "616\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "617\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "618\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "619\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "620\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "621\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "622\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "623\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "624\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "625\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "626\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "627\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "628\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "629\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "630\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "631\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "632\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "633\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "634\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "635\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "636\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "637\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "638\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "639\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "640\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "641\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "642\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "643\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "644\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "645\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "646\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "647\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "648\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "649\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "650\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "651\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "652\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "653\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "654\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "655\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "656\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "657\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "658\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "659\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "660\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "661\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "662\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "663\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "664\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "665\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "666\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "667\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "668\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "669\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "670\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "671\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "672\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "673\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: \n",
      "0.4927385892116183\n",
      "675\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "676\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "677\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "678\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "679\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "680\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "681\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "682\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "683\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "684\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "685\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "686\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "687\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "688\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "689\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "690\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "691\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "692\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "693\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "694\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "695\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "696\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "697\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "698\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "699\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "700\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "701\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "702\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "703\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "704\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "705\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "706\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "707\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "708\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "709\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "710\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "711\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "712\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "713\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "714\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "715\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "716\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "717\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "718\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "719\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "720\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "721\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "722\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "723\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "724\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "725\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "726\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "727\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "728\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "729\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "730\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "731\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "732\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "733\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "734\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "735\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "736\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "737\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "738\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "739\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "740\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "741\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "742\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "743\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "744\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "745\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "746\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "747\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "748\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "749\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "750\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "751\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "752\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "753\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "754\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "755\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "756\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "757\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "758\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "759\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "760\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "761\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "762\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "763\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "764\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "765\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "766\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "767\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "768\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "769\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "770\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "771\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "772\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "773\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "774\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "775\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "776\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "777\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "778\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "779\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "780\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "781\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "782\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "783\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "784\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "785\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "786\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "787\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "788\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "789\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "790\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "791\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "792\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "793\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "794\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "795\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "796\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "797\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "798\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "799\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "800\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "801\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "802\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "803\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "804\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "805\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "806\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "807\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "808\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "809\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "810\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "811\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "812\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "813\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "814\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "815\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "816\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "817\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "818\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "819\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "820\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "821\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "822\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "823\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "824\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "825\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "826\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "827\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "828\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "829\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "830\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "831\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "832\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "833\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "834\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "835\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "836\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "837\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "838\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "839\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "840\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "841\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "842\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "843\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "844\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "845\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "846\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "847\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "848\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "849\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "850\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "851\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "852\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "853\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "854\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "855\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "856\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "857\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "858\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "859\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "860\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "861\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "862\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "863\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "864\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "865\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "866\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "867\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "868\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "869\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "870\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "871\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "872\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "873\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "874\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "875\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "876\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "877\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "878\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "879\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "880\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "881\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "882\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "883\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "884\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "885\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "886\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "887\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "888\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "889\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "890\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "891\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "892\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "893\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "894\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "895\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "896\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "897\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: \n",
      "0.4927385892116183\n",
      "899\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "900\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "901\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "902\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "903\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "904\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "905\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "906\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "907\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "908\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "909\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "910\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "911\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "912\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "913\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "914\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "915\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "916\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "917\n",
      "Train error: \n",
      "0.4927385892116183\n",
      "918\n"
     ]
    }
   ],
   "source": [
    "epochs = 20000\n",
    "lr1 = 0.01/964*np.ones(epochs)\n",
    "# =========Write your code below ==============\n",
    "err_tst, err_trn, loss_trn, parameters = nn_model1(X_trn, X_tst, Y_trn, Y_tst, 20, 2, epochs, \"ReLU\", lr1)\n",
    "# =============================================\n",
    "plt.figure(1, figsize=(12, 8))\n",
    "plt.plot(range(epochs), loss_trn, '-', color='orange',linewidth=2, label='training loss (lr = 0.01/964)')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Cross entropy error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Learning with fixed learning rate.\n",
    "Using ReLU as your activation function, implement a learning algorithm with fixed learning rate $\\eta = 0.01$ at each step. Plot the training and testing error (classification error) you get at each epoch. Justify your plot. (Set the number of hidden neurons in the hidden layer to be 20 for problem 1-3, for all problems below, set epochs = 20000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000\n",
    "lr1 = 0.01/964*np.ones(epochs)\n",
    "# =========Write your code below ==============\n",
    "err_tst, err_trn, loss_trn, parameters = nn_model1(X_trn, X_tst, Y_trn, Y_tst, 20, 2, epochs, \"ReLU\", lr1)\n",
    "\n",
    "# =============================================\n",
    "plt.figure(1, figsize=(12, 8))\n",
    "plt.plot(range(epochs), err_trn, '-', color='orange',linewidth=2, label='training error (lr = 0.01/964)')\n",
    "plt.plot(range(epochs), err_tst, '-b', linewidth=2, label='test error (lr = 0.01/964)')\n",
    "\n",
    "\n",
    "plt.title('ReLU(Learning rate=0.01/964)')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Explanation for Problem 1 </font> ##\n",
    "<font color='blue'> At a learning rate of $\\eta = 0.01/984$ with the ReLU activation function, the training error and testing error are decreasing very slowly for the first 2400 epochs. The errors for both training and testing hover around 50%, with the training error slightly lower (around 49/48%). Beyond 2500 epochs, we see significant reduction in the training and testing errors from 50% to 17% at 5000 epochs. Beyond 10000 epochs, the testing and training errors are convering to a value of 17% and 13% respectively. At the end, the training error reaches a minimum of 12%, resulting in a overall end training accuracy of 88%. The overall end testing accuracy is around 82%. As expected, the training error is lower than the testing error, as the model is expected to perform on data it has already seen.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: \n",
    "Using ReLU as your activation function, change the learning rate to $\\eta = 0.1$. Plot the plots on the same figure as in problem 1. Compare the plots and justify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000\n",
    "lr2 = 0.1/964*np.ones(epochs)\n",
    "# =========Write your code below ==============\n",
    "err_tst2, err_trn2, loss_trn2, parameters2 = nn_model1(X_trn, X_tst, Y_trn, Y_tst, 20, 2, epochs, \"ReLU\", lr2)\n",
    "# =============================================\n",
    "plt.figure(2, figsize=(12, 8))\n",
    "# Classification errors for learning rate = 0.01/964, Relu Activation, n_hidden = 20\n",
    "plt.plot(range(epochs), err_trn, '-', color='orange',linewidth=2, label='training error (lr = 0.01/964)')\n",
    "plt.plot(range(epochs), err_tst, '-b', linewidth=2, label='test error (lr = 0.01/964)')\n",
    "\n",
    "# Classification errors for learning rate = 0.1/964, Relu Activation, n_hidden = 20\n",
    "plt.plot(range(epochs), err_trn2, '-', linewidth=2, label='training error (lr = 0.1/964)')\n",
    "plt.plot(range(epochs), err_tst2, '-b', color='yellow', linewidth=2,  label='test error (lr = 0.1/964)')\n",
    "\n",
    "plt.title('ReLU(Learning rate=0.1/964)')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Explanation for Problem 2 </font> ##\n",
    "<font color='blue'> At a learning rate of $\\eta = 0.1/984$ with the ReLU activation function, the training error and testing error are decreasing very quickly for the first 1000 epochs. Beyond 200 epochs, we see significant reduction in the training and testing errors from 50% to 15% at 1000 epochs. Beyond 1000 epochs, the testing errors are convering to a value of 17%, before rising back up to around 21%. The training error continues to decline steadily, converging to an error of around 13% between epochs 2500 and 10000. Beyond 10000 epochs, the training error continues to decline to a final training error of 5%. This may be an indication of overfitting, as the training error is significantly lower than the testing error, indicating that the model has \"memorized\" the training data. Comparing this learning rate of 0.1/984 to a learning rate of 0.01/984, we see that the learning rate of 0.01/984 achieves better testing accuracy, while a learning rate of 0.1/984 achieves a better training accuracy.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Learning with variable learning rate.\n",
    "Using ReLU as your activation function, implement a learning algorithm with variable learning rate $\\eta = \\frac1{\\sqrt{i+1}}$ at the $i$th step. Plot the training and testing error you get at each iteration and compare it with the plots you get previously. Justify your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.array(range(epochs))\n",
    "lr3 = 1/np.sqrt(indices + 1)*(1.0/964)\n",
    "# =========Write your code below ==============\n",
    "err_tst3, err_trn3, loss_trn3, parameters3 = nn_model1(X_trn, X_tst, Y_trn, Y_tst, 20, 2, epochs, \"ReLU\", lr3)\n",
    "# =============================================\n",
    "plt.figure(3, figsize=(12, 8))\n",
    "# Classification errors for learning rate = 0.01/964, Relu Activation, n_hidden = 20\n",
    "plt.plot(range(epochs), err_trn, '-', color='orange',linewidth=2, label='training error (lr = 0.01/964)')\n",
    "plt.plot(range(epochs), err_tst, '-b', linewidth=2, label='test error (lr = 0.01/964)')\n",
    "\n",
    "# Classification errors for learning rate = 0.1/964, Relu Activation, n_hidden = 20\n",
    "plt.plot(range(epochs), err_trn2, '-', color='red', linewidth=2, label='training error (lr = 0.1/964)')\n",
    "plt.plot(range(epochs), err_tst2, '-b', color='yellow', linewidth=2, label='test error (lr = 0.1/964)')\n",
    "\n",
    "# Classification errors for variable learning rate, Relu Activation, n_hidden = 20\n",
    "plt.plot(range(epochs), err_trn3, '-', color='purple', linewidth=2, label='training error (unfixed lr)')\n",
    "plt.plot(range(epochs), err_tst3, '-b', color='green', linewidth=2, label='test error (unfixed lr)')\n",
    "plt.title('ReLU(Variable learning rate)')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Explanation for Problem 3 </font> ##\n",
    "<font color='blue'> Using a variable learning rate with the ReLU activation function, both the training and testing error plots seem superior than the previous two configurations. The variable learning rate configuration converges to a training error of 13% beyond 5000 epochs and does not vary too much beyond that point. The testing error also converges to an error of 17% beyond 5000 epochs and is almost completely flat beyond that. The final testing error mimics that of the $\\eta = 0.01/984$ configuration. The use of adaptive learning rate allows the rate to be quite high initially and then decrease once the model starts to converge, which helps the model learn the correct weights optimally.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Larger hidden layer.\n",
    "Change the number of neurons in the hidden layer to be $50$. Redo the experiment in problem 1. Plot all four plots in the same figure and justify your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden2 = 50\n",
    "# =========Write your code below ==============\n",
    "err_tst4, err_trn4, loss_trn4, parameters4 = nn_model1(X_trn, X_tst, Y_trn, Y_tst, num_hidden2, 2, epochs, \"ReLU\", lr1)\n",
    "\n",
    "# =============================================\n",
    "plt.figure(4, figsize=(12, 8))\n",
    "# Classification errors for learning rate = 0.01/964, Relu Activation, n_hidden = 20\n",
    "plt.plot(range(epochs), err_trn, '-', color='orange',linewidth=2, label='training error (#hidden = 20)')\n",
    "plt.plot(range(epochs), err_tst, '-b', linewidth=2, label='test error (#hidden = 20)')\n",
    "\n",
    "# Classification errors for learning rate = 0.01/964, Relu Activation, n_hidden = 50\n",
    "plt.plot(range(epochs), err_trn4, '-', color='red', linewidth=2, label='training error (#hidden = 50)')\n",
    "plt.plot(range(epochs), err_tst4, '-b', color='grey', linewidth=2, label='test error (#hidden = 50)')\n",
    "\n",
    "plt.title('ReLU(Learning rate=0.01/964)')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Explanation for Problem 4 </font> ##\n",
    "<font color='blue'> By using 50 neurons in the hidden layer instead of 20, we see a slight improvement in the training accuracy, however not by too much. For the testing accuracy, the behavior is almost identical to that of the configuration with 20 neurons. Thus, the addition of more neurons, thereby increasing computationally complexity, does not seem to improve performance in this case by too much.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Sigmoid Activation.\n",
    "Change the activation function to be Sigmoid function. Redo the experiment in problem 1. Plot all four plots in the same figure and justify your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========Write your code below ==============\n",
    "epochs = 20000\n",
    "lr1 = 0.01/964*np.ones(epochs)\n",
    "err_tst5, err_trn5, loss_trn5, parameters5 = nn_model1(X_trn, X_tst, Y_trn, Y_tst, 20, 2, epochs, \"Sigmoid\", lr1)\n",
    "\n",
    "# =============================================\n",
    "# Classification errors for learning rate = 0.01/964, Relu Activation, n_hidden = 20\n",
    "plt.figure(5, figsize=(12, 8))\n",
    "plt.plot(range(epochs), err_trn, '-', color='orange',linewidth=2, label='training error (ReLU)')\n",
    "plt.plot(range(epochs), err_tst, '-b', linewidth=2, label='test error (ReLU)')\n",
    "\n",
    "# Classification errors for learning rate = 0.01/964, Sigmoid Activation, n_hidden = 20\n",
    "plt.plot(range(epochs), err_trn5, '-', color='red',  linewidth=2, label='training error (Sigmoid)')\n",
    "plt.plot(range(epochs), err_tst5, '-b', color='green', linewidth=2, label='test error (Sigmoid)')\n",
    "\n",
    "plt.title('Learning rate=0.01/964')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Explanation for Problem 5 </font> ##\n",
    "<font color='blue'> By using a sigmoid activation function instead of a ReLU activation function, the model seems to learn extremely slow. In the first 10000 epochs, the model with Sigmoid activation does not see much change in training and testing error at all. Beyond 10000 epochs, the training and testing errors starts to decrease to around 18% at an epoch of 15000. Beyond 15000 epochs, the model starts to converge. Comparing this with the ReLU model, we see that the ReLU model shows better performance and faster convergence on the training set (lower training error) and faster convergence on the testing set. At the end, near 17500 epochs, the testing error of the sigmoid model is actally slightly lower than the ReLU model. Overall, the ReLU activation function is superior, as it is more computationally efficient and demonstrates better convergence behavior. The Sigmoid model takes much longer to compute and does not converge until a very high epoch count.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
