{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Classification\n",
    "\n",
    "In this assignment, we will use logistic regression to judge the quality of wines. The dataset is taken from UCI machine learning repository. For description of the dataset, see [here](https://archive.ics.uci.edu/ml/datasets/wine+quality).\n",
    "\n",
    "Attributes of the dataset are listed as following:\n",
    "1. fixed acidity \n",
    "2. volatile acidity \n",
    "3. citric acid \n",
    "4. residual sugar \n",
    "5. chlorides \n",
    "6. free sulfur dioxide \n",
    "7. total sulfur dioxide \n",
    "8. density \n",
    "9. pH \n",
    "10. sulphates \n",
    "11. alcohol \n",
    "\n",
    "Output variable (based on sensory data): \n",
    "12. quality (score between 0 and 10)\n",
    "\n",
    "The following code loads the dataset, and the dataset looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4731</th>\n",
       "      <td>5.3</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.38</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.031</td>\n",
       "      <td>53.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.99321</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.46</td>\n",
       "      <td>11.7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.58</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.044</td>\n",
       "      <td>42.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0.301</td>\n",
       "      <td>24.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.99930</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.48</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3296</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.42</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.044</td>\n",
       "      <td>60.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>0.99562</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.48</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.25</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.049</td>\n",
       "      <td>59.5</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.99500</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.38</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3640</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.047</td>\n",
       "      <td>42.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.99283</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.56</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.27</td>\n",
       "      <td>10.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>31.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.99815</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.30</td>\n",
       "      <td>9.3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>20.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.99000</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.56</td>\n",
       "      <td>12.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>7.7</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.27</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.063</td>\n",
       "      <td>39.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.99690</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.63</td>\n",
       "      <td>9.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.026</td>\n",
       "      <td>29.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.99100</td>\n",
       "      <td>3.02</td>\n",
       "      <td>0.78</td>\n",
       "      <td>12.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "4731            5.3              0.31         0.38            10.5      0.031   \n",
       "937             6.1              0.36         0.58            15.0      0.044   \n",
       "1217            8.0              0.61         0.38            12.1      0.301   \n",
       "3296            6.6              0.28         0.42             8.2      0.044   \n",
       "4524            6.6              0.16         0.25             9.8      0.049   \n",
       "3640            6.8              0.19         0.33             4.9      0.047   \n",
       "785             7.6              0.30         0.27            10.6      0.039   \n",
       "393             7.3              0.24         0.43             2.0      0.021   \n",
       "562             7.7              0.34         0.27             8.8      0.063   \n",
       "1285            7.8              0.16         0.41             1.7      0.026   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "4731                 53.0                 140.0  0.99321  3.34       0.46   \n",
       "937                  42.0                 115.0  0.99780  3.15       0.51   \n",
       "1217                 24.0                 220.0  0.99930  2.94       0.48   \n",
       "3296                 60.0                 196.0  0.99562  3.14       0.48   \n",
       "4524                 59.5                 137.0  0.99500  3.16       0.38   \n",
       "3640                 42.0                 130.0  0.99283  3.12       0.56   \n",
       "785                  31.0                 119.0  0.99815  3.27       0.30   \n",
       "393                  20.0                  69.0  0.99000  3.08       0.56   \n",
       "562                  39.0                 184.0  0.99690  3.09       0.63   \n",
       "1285                 29.0                 140.0  0.99100  3.02       0.78   \n",
       "\n",
       "      alcohol  quality  \n",
       "4731     11.7        6  \n",
       "937       9.0        5  \n",
       "1217      9.2        5  \n",
       "3296      9.4        5  \n",
       "4524     10.0        6  \n",
       "3640     11.0        6  \n",
       "785       9.3        6  \n",
       "393      12.2        6  \n",
       "562       9.2        6  \n",
       "1285     12.5        6  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "#train = np.genfromtxt('wine_training1.txt', delimiter=',')\n",
    "red = pd.read_csv('winequality-red.csv')\n",
    "white = pd.read_csv('winequality-white.csv')\n",
    "red = shuffle(red, random_state = 10)\n",
    "white = shuffle(white, random_state = 10)\n",
    "red.head(10)\n",
    "white.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "To get this into a binary classification task. We split the quality into a binary feature *good* or *bad* depending on whether the quality is larger than 6 or not.\n",
    "\n",
    "Next we randomly pick $70\\%$ of the data to be our training set and the remaining for testing for both red and white wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4731     True\n",
       "937     False\n",
       "1217    False\n",
       "3296    False\n",
       "4524     True\n",
       "3640     True\n",
       "785      True\n",
       "393      True\n",
       "562      True\n",
       "1285     True\n",
       "Name: quality, dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_red = red.iloc[:, :-1]\n",
    "y_red = red.iloc[:, -1] >= 6\n",
    "\n",
    "X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.3, random_state = 0)\n",
    "\n",
    "X_white = white.iloc[:, :-1]\n",
    "y_white = white.iloc[:, -1] >= 6\n",
    "X_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.3, random_state = 0)\n",
    "\n",
    "#y_red.head(10)\n",
    "y_white.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 Logistic Regression for Red Wine\n",
    "\n",
    "Using scikit learn, train a Logistic Regression classifier using 'X_trn_red, y_trn_red'. Use the\n",
    "solver sag, which stands for Stochastic Average Gradient. Set max iteration to be 10000. Test the model on X_test_red. Output the testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing error for red wine is: 0.27291666666666664.\n"
     ]
    }
   ],
   "source": [
    "#========Your code here ======\n",
    "error_red = 0\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(penalty='none',max_iter=10000,solver='sag')\n",
    "# fit our training data \n",
    "logreg.fit(X_train_red,y_train_red)\n",
    "test_predict = logreg.predict(X_test_red)\n",
    "testSize = len(X_test_red)\n",
    "misclassify = 0\n",
    "\n",
    "for i, j in zip(y_test_red, test_predict):\n",
    "    if i!=j:\n",
    "        misclassify += 1\n",
    "error_red = float(misclassify)/float(testSize)      \n",
    "#========================\n",
    "print('The testing error for red wine is: ' + str(error_red) + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 Logistic Regression for White Wine\n",
    "\n",
    "Using scikit learn, train a Logistic Regression classifier using 'X_trn_white, y_trn_white'. Use the\n",
    "solver sag, which stands for Stochastic Average Gradient. Set max iteration to be 10000. Test the model on X_test_white. Output the testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing error for white wine is: 0.25918367346938775.\n"
     ]
    }
   ],
   "source": [
    "#========Your code here ======\n",
    "error_white = 0\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(penalty='none',max_iter=10000,solver='sag')\n",
    "# fit our training data \n",
    "logreg.fit(X_train_white,y_train_white)\n",
    "test_predict = logreg.predict(X_test_white)\n",
    "testSize = len(X_test_white)\n",
    "misclassify = 0\n",
    "\n",
    "for i, j in zip(y_test_white, test_predict):\n",
    "    if i != j:\n",
    "        misclassify += 1\n",
    "error_white = float(misclassify)/float(testSize)   \n",
    "#========================\n",
    "print('The testing error for white wine is: ' + str(error_white) + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 \n",
    "Use the model you trained using 'X_trn_white, y_trn_white' to test on 'X_test_red' and use the model you trained on 'X_test_white'. Print out the errors and compare with previous results. Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing error for red wine using white wine training data is: 0.35833333333333334.\n",
      "The testing error for white wine using red wine training data is: 0.3326530612244898.\n"
     ]
    }
   ],
   "source": [
    "#========Your code here ======\n",
    "error_white = 0\n",
    "error_red = 0\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# fit the white data for white classifier\n",
    "logregwhite = LogisticRegression(penalty='none',max_iter=10000,solver='sag')\n",
    "logregwhite.fit(X_train_white,y_train_white)\n",
    "# fit the red data for white classifier\n",
    "logregred = LogisticRegression(penalty='none',max_iter=10000,solver='sag')\n",
    "logregred.fit(X_train_red,y_train_red)\n",
    "\n",
    "# generate the predictions for respective test sets\n",
    "test_predictRedUsingWhite = logregwhite.predict(X_test_red)\n",
    "test_predictWhiteUsingRed = logregred.predict(X_test_white)\n",
    "\n",
    "misclassifyred = 0\n",
    "misclassifywhite = 0\n",
    "testSizeRed = len(test_predictRedUsingWhite)\n",
    "testSizeWhite = len(test_predictWhiteUsingRed)\n",
    "\n",
    "# error calculation for white, red classifier on red, white training sets\n",
    "for i, j in zip(y_test_red, test_predictRedUsingWhite):\n",
    "    if i != j:\n",
    "        misclassifyred += 1\n",
    "        \n",
    "for i, j in zip(y_test_white, test_predictWhiteUsingRed):\n",
    "    if i!=j:\n",
    "        misclassifywhite += 1\n",
    "        \n",
    "error_red = float(misclassifyred)/float(testSizeRed)  \n",
    "error_white = float(misclassifywhite)/float(testSizeWhite) \n",
    "\n",
    "#========================\n",
    "print('The testing error for red wine using white wine training data is: ' + str(error_red) + '.')\n",
    "print('The testing error for white wine using red wine training data is: ' + str(error_white) + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>  Explanation for Problem 3 </font> ###\n",
    "<font color='blue'>  The testing error for red wine and white wine are both higher when the reversed models are used. The testing error for red and white wine respectively when reversed models are used are: 35.8% and 33.2%. Before, when the right models are used, the errors for red and white wine are: 27.2% and 25.9%. A likely explanation is that although red and white wine features may be somewhat similar, when applying the wrong model, the classifier suffers some inaccuracy due to the fact that the classifier cannot apply its learned information on a different set of wine categories. An analogy in real life would be using information learned for rating quality of oranges on apples. Although symmetry and weight can be somewhat applied, the other features simply do not map well together.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 The effect of regularization\n",
    "Using red wine dataset. Implement logistic regression in sklearn, using $\\ell_2$ regularization with regularizer value C in the set $\\{0.00001 \\times 4^i: i = 0,1,2,..., 15\\}$. (The regularization parameter is 'C' in scikit-learn, which is the inverse of $\\lambda$ we see in class). Plot the training error and test error with respect to the regularizer value. Explain what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x124c2a750>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "N = np.array(range(0,15))\n",
    "alpha = 0.00001*(4**N)\n",
    "error_trn = np.zeros(15)\n",
    "error_tst = np.zeros(15)\n",
    "#========Your code here ======\n",
    "# iterate over the l5 regularization values in set {0.00001√ó4ùëñ:ùëñ=0,1,2,...,14}\n",
    "for i in range(0,len(N)):\n",
    "    logreg = LogisticRegression(max_iter=10000,solver='sag',C=alpha[i])\n",
    "    # fit our training data \n",
    "    logreg.fit(X_train_red,y_train_red)\n",
    "    # predict for training and test data\n",
    "    train_predict = logreg.predict(X_train_red)\n",
    "    test_predict = logreg.predict(X_test_red)\n",
    "    testSize = len(X_test_red)\n",
    "    trainSize = len(X_train_red)\n",
    "    misclassifytest = 0\n",
    "    misclassifytrain = 0\n",
    "    for x, y in zip(y_train_red, train_predict):\n",
    "        if x!=y:\n",
    "            misclassifytrain += 1\n",
    "    error_red_train = float(misclassifytrain)/float(trainSize)  \n",
    "    error_trn[i] = error_red_train\n",
    "    for x, y in zip(y_test_red, test_predict):\n",
    "        if x!=y:\n",
    "            misclassifytest += 1\n",
    "    error_red_test = float(misclassifytest)/float(testSize)  \n",
    "    error_tst[i] = error_red_test \n",
    "#========================\n",
    "plt.figure(1)\n",
    "plt.semilogx(alpha, error_tst,label = 'Test')\n",
    "plt.semilogx(alpha, error_trn, label = 'Train')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>  Explanation for Problem 4 </font> ###\n",
    "<font color='blue'> For the training error, when the C factor is increased from an order of 10^-4 to an order of 10^2, the error dropped from 38% to 25%. This is the expected behavior. As the C factor is increased, the regularization parameter decreases because C is the inverse of the regularization factor. Thus, when we increase C, we are decreasing lambda, making the model less prone to underfitting. Similarly, for the testing error, when the C factor is increased, the error dropped from 37% to around 28%. An interesting behavior is that the tesing error increased between a C factor of 10^-4 to 10^-2, followed by a large drop after 10^-2. Overall, the training error is lower than the testing error, as the model is expected to do better on data it has seen before. The end goal is to find the regularization parameter that optimizes the model's performance.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
